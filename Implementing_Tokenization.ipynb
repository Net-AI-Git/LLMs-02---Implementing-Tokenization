{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Net-AI-Git/LLMs-02---Implementing-Tokenization/blob/main/Implementing_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_kYuBTIZ_AO",
        "outputId": "a33d57c5-a622-4b94-f7b5-cc88aa1ecb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Installing collected packages: regex, joblib, click, nltk\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6\n",
            "Collecting transformers==4.42.1\n",
            "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting filelock (from transformers==4.42.1)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.42.1)\n",
            "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2.0,>=1.17 (from transformers==4.42.1)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.1)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.42.1)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1)\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.12.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1)\n",
            "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2024.12.14)\n",
            "Downloading transformers-4.42.1-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m170.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: safetensors, numpy, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.0 numpy-1.26.4 safetensors-0.5.3 tokenizers-0.19.1 transformers-4.42.1\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from spacy) (75.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.1-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m215.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, srsly, smart-open, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.1 preshed-3.0.10 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m159.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.3.1)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2) (4.12.2)\n",
            "Collecting sympy (from torch==2.2.2)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.2.2)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2) (2025.5.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.2)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "Installing collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "Successfully installed mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.14.0 torch-2.2.2\n",
            "Collecting torchtext==0.17.2\n",
            "  Downloading torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.2 in /opt/conda/lib/python3.12/site-packages (from torchtext==0.17.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchtext==0.17.2) (2.3.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.9.86)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Downloading torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.17.2\n",
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.1\n",
            "    Uninstalling numpy-2.3.1:\n",
            "      Successfully uninstalled numpy-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install transformers==4.42.1\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install scikit-learn\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install numpy==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dph0qfYCZ_AQ",
        "outputId": "a639074c-d8eb-4dc1-9d83-581d6ef8101f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/jupyterlab/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-nja0eZ_AQ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhawD7KpZ_AR",
        "outputId": "01fb375d-e95f-42d3-fd50-0749bcc9b627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'helps', 'computers', 'understand', 'human', 'communication', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural language processing helps computers understand human communication.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GqElH7iZ_AR",
        "outputId": "d9188922-6a0a-4fb3-dc53-1e7bdcf8bd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Models', 'learn', 'language', 'patterns', '.', 'Can', 'they', 'reason', '?', 'Do', \"n't\", 'doubt', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# This showcases word_tokenize from nltk library\n",
        "\n",
        "text = \"Models learn language patterns. Can they reason? Don't doubt it.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuXiFKCYZ_AR",
        "outputId": "8a5f252b-cebd-4a82-b5ae-5b476385f618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nlp: <spacy.lang.en.English object at 0x75d5bd149a90>\n",
            "doc: GPT models can't process infinite contexts. They don't have unlimited memory.\n",
            "Tokens: ['GPT', 'models', 'ca', \"n't\", 'process', 'infinite', 'contexts', '.', 'They', 'do', \"n't\", 'have', 'unlimited', 'memory', '.']\n",
            "GPT PROPN compound\n",
            "models NOUN nsubj\n",
            "ca AUX aux\n",
            "n't PART neg\n",
            "process VERB ROOT\n",
            "infinite PROPN compound\n",
            "contexts PROPN dobj\n",
            ". PUNCT punct\n",
            "They PRON nsubj\n",
            "do AUX aux\n",
            "n't PART neg\n",
            "have VERB ROOT\n",
            "unlimited ADJ amod\n",
            "memory NOUN dobj\n",
            ". PUNCT punct\n"
          ]
        }
      ],
      "source": [
        "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
        "\n",
        "text = \"GPT models can't process infinite contexts. They don't have unlimited memory.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"nlp:\", nlp)\n",
        "doc = nlp(text)\n",
        "print(\"doc:\", doc)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hczdxtJfZ_AS",
        "outputId": "1f33fb90-ffbb-4e7b-c3eb-652c022666f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['BERT', 'uses', 'bidirectional', 'attention', 'mechanisms', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"BERT uses bidirectional attention mechanisms.\"\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4195b438c5024ebc8d8ad632ff9ff935",
            "1054d73da27c432d8231dd9ed53596b4",
            "3d68aea4ba9a4f858e4f87b75b2a0d85",
            "813e593df7514a639ca24aa3621e306a"
          ]
        },
        "id": "5r4J-IhMZ_AT",
        "outputId": "e4248339-3753-4cb7-ba0b-19dea9236204"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4195b438c5024ebc8d8ad632ff9ff935",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1054d73da27c432d8231dd9ed53596b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d68aea4ba9a4f858e4f87b75b2a0d85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "813e593df7514a639ca24aa3621e306a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['bert',\n",
              " 'uses',\n",
              " 'bid',\n",
              " '##ire',\n",
              " '##ction',\n",
              " '##al',\n",
              " 'attention',\n",
              " 'mechanisms',\n",
              " '.']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"BERT uses bidirectional attention mechanisms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a70258172c7f43d88c3b9b0a8e05b56d",
            "9e1067f97d6d451d98390d39f868ce2e",
            "8b7ad90dfce04c9d820932085b923d1a"
          ]
        },
        "id": "W0AKB18iZ_AT",
        "outputId": "c72a740e-04f9-44ca-953a-a05a2cb1c4d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a70258172c7f43d88c3b9b0a8e05b56d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e1067f97d6d451d98390d39f868ce2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b7ad90dfce04c9d820932085b923d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['▁B', 'ERT', '▁uses', '▁bi', 'directional', '▁attention', '▁mechanisms', '.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer.tokenize(\"BERT uses bidirectional attention mechanisms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OA6U0xDZ_AU"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWo2Jr5DZ_AU"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk9FjMBaZ_AV"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM-mjH_zZ_AV",
        "outputId": "eb459d44-2542-468f-b2a3-06a798e9d747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(dataset[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyWeRQ5bZ_AW"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for  _,text in data_iter:\n",
        "        yield tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0-UELyfZ_AW"
      },
      "outputs": [],
      "source": [
        "my_iterator = yield_tokens(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-STE7mmCZ_AW",
        "outputId": "88e50d97-4b57-4dbe-af0f-9ca8bfb13f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(my_iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPC4bwuvZ_AX"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VguQMXBBZ_AX",
        "outputId": "a6fc8c58-560f-4df0-974b-7e735098d1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
            "Token Indices: [11, 15, 2]\n"
          ]
        }
      ],
      "source": [
        "def get_tokenized_sentence_and_indices(iterator):\n",
        "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
        "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
        "    return tokenized_sentence, token_indices\n",
        "\n",
        "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
        "next(my_iterator)\n",
        "\n",
        "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
        "print(\"Token Indices:\", token_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6vhD22XZ_AX",
        "outputId": "423b9739-abf4-4beb-da94-cd1316905726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lines after adding special tokens:\n",
            " [['<bos>', 'Multimodal', 'models', 'process', 'multiple', 'formats', '<eos>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'They', 'do', \"n't\", 'understand', 'visual', 'content', '<eos>', '<pad>', '<pad>'], ['<bos>', 'Cross', '-', 'modal', 'attention', 'connects', 'different', 'inputs', '.', '<eos>']]\n",
            "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '-', '.', 'Cross', 'Multimodal', 'They', 'attention', 'connects', 'content', 'different', 'do', 'formats', 'inputs', 'modal', 'models', 'multiple', \"n't\", 'process', 'understand', 'visual']\n",
            "Token IDs for 'tokenization': {'visual': 22, 'understand': 21, 'process': 20, \"n't\": 19, 'multiple': 18, 'models': 17, 'modal': 16, 'inputs': 15, 'formats': 14, 'do': 13, '<unk>': 0, '<pad>': 1, '-': 4, '<bos>': 2, '<eos>': 3, 'connects': 10, '.': 5, 'Multimodal': 7, 'content': 11, 'Cross': 6, 'They': 8, 'attention': 9, 'different': 12}\n"
          ]
        }
      ],
      "source": [
        "lines = [\"Multimodal models process multiple formats\",\n",
        "         \"They don't understand visual content\",\n",
        "         \"Cross-modal attention connects different inputs.\"]\n",
        "\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "tokens = []\n",
        "max_length = 0\n",
        "\n",
        "for line in lines:\n",
        "    tokenized_line = tokenizer_en(line)\n",
        "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
        "    tokens.append(tokenized_line)\n",
        "    max_length = max(max_length, len(tokenized_line))\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
        "\n",
        "print(\"Lines after adding special tokens:\\n\", tokens)\n",
        "\n",
        "# Build vocabulary without unk_init\n",
        "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Vocabulary and Token Ids\n",
        "print(\"Vocabulary:\", vocab.get_itos())\n",
        "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9CAMhQZ_AY",
        "outputId": "b836df41-7460-468a-d5d2-f755740dc6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs for new line: [2, 0, 0, 0, 0, 0, 9, 0, 5, 3]\n"
          ]
        }
      ],
      "source": [
        "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
        "\n",
        "# Tokenize the new line\n",
        "tokenized_new_line = tokenizer_en(new_line)\n",
        "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
        "\n",
        "# Pad the new line to match the maximum length of previous lines\n",
        "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
        "\n",
        "# Convert tokens to IDs and handle unknown words\n",
        "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
        "\n",
        "# Example usage\n",
        "print(\"Token IDs for new line:\", new_line_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEsn1h_5Z_AY"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
        "Eager to see where this learning path takes me next!\"\n",
        "\"\"\"\n",
        "\n",
        "# Counting and displaying tokens and their frequency\n",
        "from collections import Counter\n",
        "def show_frequencies(tokens, method_name):\n",
        "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoFq6GJMZ_AY",
        "outputId": "4cb6cc72-3c28-4c95-a4ec-2adc2e50239e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
            "Time Taken: 0:00:00.000506 seconds\n",
            "\n",
            "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
            "\n",
            "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
            "Time Taken: 0:00:00.022999 seconds\n",
            "\n",
            "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
            "Time Taken: 0:00:00.001050 seconds\n",
            "\n",
            "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
            "Time Taken: 0:00:00.000706 seconds\n",
            "\n",
            "XLNet Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import BertTokenizer, XLNetTokenizer\n",
        "from datetime import datetime\n",
        "\n",
        "# NLTK Tokenization\n",
        "start_time = datetime.now()\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "nltk_time = datetime.now() - start_time\n",
        "\n",
        "# SpaCy Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "start_time = datetime.now()\n",
        "spacy_tokens = [token.text for token in nlp(text)]\n",
        "spacy_time = datetime.now() - start_time\n",
        "\n",
        "# BertTokenizer Tokenization\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "start_time = datetime.now()\n",
        "bert_tokens = bert_tokenizer.tokenize(text)\n",
        "bert_time = datetime.now() - start_time\n",
        "\n",
        "# XLNetTokenizer Tokenization\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "start_time = datetime.now()\n",
        "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
        "xlnet_time = datetime.now() - start_time\n",
        "\n",
        "# Display tokens, time taken for each tokenizer, and token frequencies\n",
        "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
        "show_frequencies(nltk_tokens, \"NLTK\")\n",
        "\n",
        "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
        "show_frequencies(spacy_tokens, \"SpaCy\")\n",
        "\n",
        "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
        "show_frequencies(bert_tokens, \"Bert\")\n",
        "\n",
        "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
        "show_frequencies(xlnet_tokens, \"XLNet\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "b07eeb64da639d921152399e4e12fe8c69fe4147591cddf616f678d21334129b",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}