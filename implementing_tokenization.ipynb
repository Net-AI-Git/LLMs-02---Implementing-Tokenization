{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Net-AI-Git/LLMs-02---Implementing-Tokenization/blob/main/implementing_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_kYuBTIZ_AO"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install transformers==4.42.1\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install scikit-learn\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install numpy==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dph0qfYCZ_AQ",
        "outputId": "a639074c-d8eb-4dc1-9d83-581d6ef8101f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/jupyterlab/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-nja0eZ_AQ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhawD7KpZ_AR",
        "outputId": "01fb375d-e95f-42d3-fd50-0749bcc9b627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'helps', 'computers', 'understand', 'human', 'communication', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural language processing helps computers understand human communication.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GqElH7iZ_AR",
        "outputId": "d9188922-6a0a-4fb3-dc53-1e7bdcf8bd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Models', 'learn', 'language', 'patterns', '.', 'Can', 'they', 'reason', '?', 'Do', \"n't\", 'doubt', 'it', '.']\n"
          ]
        }
      ],
      "source": [
        "# This showcases word_tokenize from nltk library\n",
        "\n",
        "text = \"Models learn language patterns. Can they reason? Don't doubt it.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuXiFKCYZ_AR",
        "outputId": "8a5f252b-cebd-4a82-b5ae-5b476385f618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nlp: <spacy.lang.en.English object at 0x75d5bd149a90>\n",
            "doc: GPT models can't process infinite contexts. They don't have unlimited memory.\n",
            "Tokens: ['GPT', 'models', 'ca', \"n't\", 'process', 'infinite', 'contexts', '.', 'They', 'do', \"n't\", 'have', 'unlimited', 'memory', '.']\n",
            "GPT PROPN compound\n",
            "models NOUN nsubj\n",
            "ca AUX aux\n",
            "n't PART neg\n",
            "process VERB ROOT\n",
            "infinite PROPN compound\n",
            "contexts PROPN dobj\n",
            ". PUNCT punct\n",
            "They PRON nsubj\n",
            "do AUX aux\n",
            "n't PART neg\n",
            "have VERB ROOT\n",
            "unlimited ADJ amod\n",
            "memory NOUN dobj\n",
            ". PUNCT punct\n"
          ]
        }
      ],
      "source": [
        "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
        "\n",
        "text = \"GPT models can't process infinite contexts. They don't have unlimited memory.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"nlp:\", nlp)\n",
        "doc = nlp(text)\n",
        "print(\"doc:\", doc)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hczdxtJfZ_AS",
        "outputId": "1f33fb90-ffbb-4e7b-c3eb-652c022666f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['BERT', 'uses', 'bidirectional', 'attention', 'mechanisms', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"BERT uses bidirectional attention mechanisms.\"\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4195b438c5024ebc8d8ad632ff9ff935",
            "1054d73da27c432d8231dd9ed53596b4",
            "3d68aea4ba9a4f858e4f87b75b2a0d85",
            "813e593df7514a639ca24aa3621e306a"
          ]
        },
        "id": "5r4J-IhMZ_AT",
        "outputId": "e4248339-3753-4cb7-ba0b-19dea9236204"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4195b438c5024ebc8d8ad632ff9ff935",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1054d73da27c432d8231dd9ed53596b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d68aea4ba9a4f858e4f87b75b2a0d85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "813e593df7514a639ca24aa3621e306a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['bert',\n",
              " 'uses',\n",
              " 'bid',\n",
              " '##ire',\n",
              " '##ction',\n",
              " '##al',\n",
              " 'attention',\n",
              " 'mechanisms',\n",
              " '.']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"BERT uses bidirectional attention mechanisms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a70258172c7f43d88c3b9b0a8e05b56d",
            "9e1067f97d6d451d98390d39f868ce2e",
            "8b7ad90dfce04c9d820932085b923d1a"
          ]
        },
        "id": "W0AKB18iZ_AT",
        "outputId": "c72a740e-04f9-44ca-953a-a05a2cb1c4d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a70258172c7f43d88c3b9b0a8e05b56d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e1067f97d6d451d98390d39f868ce2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b7ad90dfce04c9d820932085b923d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['▁B', 'ERT', '▁uses', '▁bi', 'directional', '▁attention', '▁mechanisms', '.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer.tokenize(\"BERT uses bidirectional attention mechanisms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OA6U0xDZ_AU"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWo2Jr5DZ_AU"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk9FjMBaZ_AV"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM-mjH_zZ_AV",
        "outputId": "eb459d44-2542-468f-b2a3-06a798e9d747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(dataset[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyWeRQ5bZ_AW"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for  _,text in data_iter:\n",
        "        yield tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0-UELyfZ_AW"
      },
      "outputs": [],
      "source": [
        "my_iterator = yield_tokens(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-STE7mmCZ_AW",
        "outputId": "88e50d97-4b57-4dbe-af0f-9ca8bfb13f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(my_iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPC4bwuvZ_AX"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VguQMXBBZ_AX",
        "outputId": "a6fc8c58-560f-4df0-974b-7e735098d1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
            "Token Indices: [11, 15, 2]\n"
          ]
        }
      ],
      "source": [
        "def get_tokenized_sentence_and_indices(iterator):\n",
        "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
        "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
        "    return tokenized_sentence, token_indices\n",
        "\n",
        "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
        "next(my_iterator)\n",
        "\n",
        "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
        "print(\"Token Indices:\", token_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6vhD22XZ_AX",
        "outputId": "423b9739-abf4-4beb-da94-cd1316905726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lines after adding special tokens:\n",
            " [['<bos>', 'Multimodal', 'models', 'process', 'multiple', 'formats', '<eos>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'They', 'do', \"n't\", 'understand', 'visual', 'content', '<eos>', '<pad>', '<pad>'], ['<bos>', 'Cross', '-', 'modal', 'attention', 'connects', 'different', 'inputs', '.', '<eos>']]\n",
            "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '-', '.', 'Cross', 'Multimodal', 'They', 'attention', 'connects', 'content', 'different', 'do', 'formats', 'inputs', 'modal', 'models', 'multiple', \"n't\", 'process', 'understand', 'visual']\n",
            "Token IDs for 'tokenization': {'visual': 22, 'understand': 21, 'process': 20, \"n't\": 19, 'multiple': 18, 'models': 17, 'modal': 16, 'inputs': 15, 'formats': 14, 'do': 13, '<unk>': 0, '<pad>': 1, '-': 4, '<bos>': 2, '<eos>': 3, 'connects': 10, '.': 5, 'Multimodal': 7, 'content': 11, 'Cross': 6, 'They': 8, 'attention': 9, 'different': 12}\n"
          ]
        }
      ],
      "source": [
        "lines = [\"Multimodal models process multiple formats\",\n",
        "         \"They don't understand visual content\",\n",
        "         \"Cross-modal attention connects different inputs.\"]\n",
        "\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "tokens = []\n",
        "max_length = 0\n",
        "\n",
        "for line in lines:\n",
        "    tokenized_line = tokenizer_en(line)\n",
        "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
        "    tokens.append(tokenized_line)\n",
        "    max_length = max(max_length, len(tokenized_line))\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
        "\n",
        "print(\"Lines after adding special tokens:\\n\", tokens)\n",
        "\n",
        "# Build vocabulary without unk_init\n",
        "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Vocabulary and Token Ids\n",
        "print(\"Vocabulary:\", vocab.get_itos())\n",
        "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9CAMhQZ_AY",
        "outputId": "b836df41-7460-468a-d5d2-f755740dc6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs for new line: [2, 0, 0, 0, 0, 0, 9, 0, 5, 3]\n"
          ]
        }
      ],
      "source": [
        "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
        "\n",
        "# Tokenize the new line\n",
        "tokenized_new_line = tokenizer_en(new_line)\n",
        "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
        "\n",
        "# Pad the new line to match the maximum length of previous lines\n",
        "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
        "\n",
        "# Convert tokens to IDs and handle unknown words\n",
        "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
        "\n",
        "# Example usage\n",
        "print(\"Token IDs for new line:\", new_line_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEsn1h_5Z_AY"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
        "Eager to see where this learning path takes me next!\"\n",
        "\"\"\"\n",
        "\n",
        "# Counting and displaying tokens and their frequency\n",
        "from collections import Counter\n",
        "def show_frequencies(tokens, method_name):\n",
        "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoFq6GJMZ_AY",
        "outputId": "4cb6cc72-3c28-4c95-a4ec-2adc2e50239e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
            "Time Taken: 0:00:00.000506 seconds\n",
            "\n",
            "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
            "\n",
            "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
            "Time Taken: 0:00:00.022999 seconds\n",
            "\n",
            "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
            "Time Taken: 0:00:00.001050 seconds\n",
            "\n",
            "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
            "Time Taken: 0:00:00.000706 seconds\n",
            "\n",
            "XLNet Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import BertTokenizer, XLNetTokenizer\n",
        "from datetime import datetime\n",
        "\n",
        "# NLTK Tokenization\n",
        "start_time = datetime.now()\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "nltk_time = datetime.now() - start_time\n",
        "\n",
        "# SpaCy Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "start_time = datetime.now()\n",
        "spacy_tokens = [token.text for token in nlp(text)]\n",
        "spacy_time = datetime.now() - start_time\n",
        "\n",
        "# BertTokenizer Tokenization\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "start_time = datetime.now()\n",
        "bert_tokens = bert_tokenizer.tokenize(text)\n",
        "bert_time = datetime.now() - start_time\n",
        "\n",
        "# XLNetTokenizer Tokenization\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "start_time = datetime.now()\n",
        "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
        "xlnet_time = datetime.now() - start_time\n",
        "\n",
        "# Display tokens, time taken for each tokenizer, and token frequencies\n",
        "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
        "show_frequencies(nltk_tokens, \"NLTK\")\n",
        "\n",
        "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
        "show_frequencies(spacy_tokens, \"SpaCy\")\n",
        "\n",
        "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
        "show_frequencies(bert_tokens, \"Bert\")\n",
        "\n",
        "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
        "show_frequencies(xlnet_tokens, \"XLNet\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "b07eeb64da639d921152399e4e12fe8c69fe4147591cddf616f678d21334129b",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}